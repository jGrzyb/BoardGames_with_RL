{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, config=None):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        # Define action and observation space\n",
    "        self.action_space = spaces.Discrete(2)  # Example: 2 discrete actions\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(\n",
    "            4,), dtype=np.float32)  # Example: 4 continuous states\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the state of the environment to an initial state\n",
    "        self.state = np.random.rand(4)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Apply action and return new state, reward, done, and info\n",
    "        self.state = np.random.rand(4)\n",
    "        reward = 1 if action == 1 else 0  # Example reward logic\n",
    "        done = np.random.rand() > 0.95  # Example termination condition\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        pass  # Optional: Implement rendering logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_creator(env_config):\n",
    "    return CustomEnv(env_config)\n",
    "\n",
    "\n",
    "register_env(\"custom_env\", env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 00:16:46,915\tINFO worker.py:1718 -- Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 00:16:51,053\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n",
      "2025-05-03 00:17:51,064\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "2025-05-03 00:17:52,820\tERROR actor_manager.py:873 -- Ray error (The actor 6cfe101d367a767713840a7c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "2025-05-03 00:17:52,821\tERROR actor_manager.py:873 -- Ray error (The actor 231123bbe50925679d50769501000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "2025-05-03 00:17:52,822\tERROR actor_manager.py:674 -- The actor 6cfe101d367a767713840a7c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "NoneType: None\n",
      "2025-05-03 00:17:52,824\tERROR actor_manager.py:674 -- The actor 231123bbe50925679d50769501000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'config': {'_disable_action_flattening': False,\n",
      "            '_disable_execution_plan_api': -1,\n",
      "            '_disable_initialize_loss_from_dummy_batch': False,\n",
      "            '_disable_preprocessor_api': False,\n",
      "            '_dont_auto_sync_env_runner_states': False,\n",
      "            '_enable_rl_module_api': -1,\n",
      "            '_env_to_module_connector': None,\n",
      "            '_fake_gpus': False,\n",
      "            '_is_atari': None,\n",
      "            '_learner_class': None,\n",
      "            '_learner_connector': None,\n",
      "            '_model_config': {},\n",
      "            '_module_to_env_connector': None,\n",
      "            '_per_module_overrides': {},\n",
      "            '_prior_exploration_config': {'type': 'StochasticSampling'},\n",
      "            '_rl_module_spec': None,\n",
      "            '_tf_policy_handles_more_than_one_loss': False,\n",
      "            '_torch_grad_scaler_class': None,\n",
      "            '_torch_lr_scheduler_classes': None,\n",
      "            '_train_batch_size_per_learner': None,\n",
      "            '_use_msgpack_checkpoints': False,\n",
      "            '_validate_config': True,\n",
      "            'action_mask_key': 'action_mask',\n",
      "            'action_space': None,\n",
      "            'actions_in_input_normalized': False,\n",
      "            'add_default_connectors_to_env_to_module_pipeline': True,\n",
      "            'add_default_connectors_to_learner_pipeline': True,\n",
      "            'add_default_connectors_to_module_to_env_pipeline': True,\n",
      "            'algorithm_config_overrides_per_module': {},\n",
      "            'always_attach_evaluation_results': -1,\n",
      "            'auto_wrap_old_gym_envs': -1,\n",
      "            'batch_mode': 'truncate_episodes',\n",
      "            'broadcast_env_runner_states': True,\n",
      "            'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>,\n",
      "            'callbacks_on_algorithm_init': None,\n",
      "            'callbacks_on_checkpoint_loaded': None,\n",
      "            'callbacks_on_env_runners_recreated': None,\n",
      "            'callbacks_on_environment_created': None,\n",
      "            'callbacks_on_episode_created': None,\n",
      "            'callbacks_on_episode_end': None,\n",
      "            'callbacks_on_episode_start': None,\n",
      "            'callbacks_on_episode_step': None,\n",
      "            'callbacks_on_evaluate_end': None,\n",
      "            'callbacks_on_evaluate_start': None,\n",
      "            'callbacks_on_sample_end': None,\n",
      "            'callbacks_on_train_result': None,\n",
      "            'checkpoint_trainable_policies_only': False,\n",
      "            'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>,\n",
      "            'clip_actions': False,\n",
      "            'clip_param': 0.3,\n",
      "            'clip_rewards': None,\n",
      "            'compress_observations': False,\n",
      "            'count_steps_by': 'env_steps',\n",
      "            'create_env_on_driver': False,\n",
      "            'create_local_env_runner': True,\n",
      "            'custom_async_evaluation_function': -1,\n",
      "            'custom_eval_function': None,\n",
      "            'custom_resources_per_env_runner': {},\n",
      "            'dataset_num_iters_per_learner': None,\n",
      "            'delay_between_env_runner_restarts_s': 60.0,\n",
      "            'disable_env_checking': False,\n",
      "            'eager_max_retraces': 20,\n",
      "            'eager_tracing': True,\n",
      "            'enable_async_evaluation': -1,\n",
      "            'enable_connectors': -1,\n",
      "            'enable_env_runner_and_connector_v2': True,\n",
      "            'enable_rl_module_and_learner': True,\n",
      "            'enable_tf1_exec_eagerly': False,\n",
      "            'entropy_coeff': 0.0,\n",
      "            'entropy_coeff_schedule': None,\n",
      "            'env': 'custom_env',\n",
      "            'env_config': {},\n",
      "            'env_runner_cls': None,\n",
      "            'env_runner_health_probe_timeout_s': 30.0,\n",
      "            'env_runner_restore_timeout_s': 1800.0,\n",
      "            'env_task_fn': -1,\n",
      "            'episode_lookback_horizon': 1,\n",
      "            'episodes_to_numpy': True,\n",
      "            'evaluation_auto_duration_max_env_steps_per_sample': 2000,\n",
      "            'evaluation_auto_duration_min_env_steps_per_sample': 100,\n",
      "            'evaluation_config': None,\n",
      "            'evaluation_duration': 10,\n",
      "            'evaluation_duration_unit': 'episodes',\n",
      "            'evaluation_force_reset_envs_before_iteration': True,\n",
      "            'evaluation_interval': None,\n",
      "            'evaluation_num_env_runners': 0,\n",
      "            'evaluation_parallel_to_training': False,\n",
      "            'evaluation_sample_timeout_s': 120.0,\n",
      "            'exploration_config': {},\n",
      "            'explore': True,\n",
      "            'export_native_model_files': False,\n",
      "            'extra_python_environs_for_driver': {},\n",
      "            'extra_python_environs_for_worker': {},\n",
      "            'fake_sampler': False,\n",
      "            'framework': 'torch',\n",
      "            'gamma': 0.99,\n",
      "            'grad_clip': None,\n",
      "            'grad_clip_by': 'global_norm',\n",
      "            'gym_env_vectorize_mode': 'SYNC',\n",
      "            'ignore_env_runner_failures': False,\n",
      "            'ignore_final_observation': False,\n",
      "            'in_evaluation': False,\n",
      "            'input': 'sampler',\n",
      "            'input_compress_columns': ['obs', 'new_obs'],\n",
      "            'input_config': {},\n",
      "            'input_filesystem': None,\n",
      "            'input_filesystem_kwargs': {},\n",
      "            'input_read_batch_size': None,\n",
      "            'input_read_episodes': False,\n",
      "            'input_read_method': 'read_parquet',\n",
      "            'input_read_method_kwargs': {},\n",
      "            'input_read_sample_batches': False,\n",
      "            'input_read_schema': {},\n",
      "            'input_spaces_jsonable': True,\n",
      "            'iter_batches_kwargs': {},\n",
      "            'keep_per_episode_custom_metrics': False,\n",
      "            'kl_coeff': 0.2,\n",
      "            'kl_target': 0.01,\n",
      "            'lambda': 1.0,\n",
      "            'learner_config_dict': {},\n",
      "            'local_gpu_idx': 0,\n",
      "            'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
      "                                      'intra_op_parallelism_threads': 8},\n",
      "            'log_gradients': True,\n",
      "            'log_level': 'WARN',\n",
      "            'log_sys_usage': True,\n",
      "            'logger_config': None,\n",
      "            'logger_creator': None,\n",
      "            'lr': 5e-05,\n",
      "            'lr_schedule': None,\n",
      "            'map_batches_kwargs': {},\n",
      "            'materialize_data': False,\n",
      "            'materialize_mapped_data': True,\n",
      "            'max_num_env_runner_restarts': 1000,\n",
      "            'max_requests_in_flight_per_aggregator_actor': 3,\n",
      "            'max_requests_in_flight_per_env_runner': 1,\n",
      "            'max_requests_in_flight_per_learner': 3,\n",
      "            'merge_env_runner_states': 'training_only',\n",
      "            'metrics_episode_collection_timeout_s': 60.0,\n",
      "            'metrics_num_episodes_for_smoothing': 100,\n",
      "            'min_sample_timesteps_per_iteration': 0,\n",
      "            'min_time_s_per_iteration': None,\n",
      "            'min_train_timesteps_per_iteration': 0,\n",
      "            'minibatch_size': 128,\n",
      "            'model': {'_disable_action_flattening': False,\n",
      "                      '_disable_preprocessor_api': False,\n",
      "                      '_time_major': False,\n",
      "                      '_use_default_native_models': -1,\n",
      "                      'always_check_shapes': False,\n",
      "                      'attention_dim': 64,\n",
      "                      'attention_head_dim': 32,\n",
      "                      'attention_init_gru_gate_bias': 2.0,\n",
      "                      'attention_memory_inference': 50,\n",
      "                      'attention_memory_training': 50,\n",
      "                      'attention_num_heads': 1,\n",
      "                      'attention_num_transformer_units': 1,\n",
      "                      'attention_position_wise_mlp_dim': 32,\n",
      "                      'attention_use_n_prev_actions': 0,\n",
      "                      'attention_use_n_prev_rewards': 0,\n",
      "                      'conv_activation': 'relu',\n",
      "                      'conv_bias_initializer': None,\n",
      "                      'conv_bias_initializer_config': None,\n",
      "                      'conv_filters': None,\n",
      "                      'conv_kernel_initializer': None,\n",
      "                      'conv_kernel_initializer_config': None,\n",
      "                      'conv_transpose_bias_initializer': None,\n",
      "                      'conv_transpose_bias_initializer_config': None,\n",
      "                      'conv_transpose_kernel_initializer': None,\n",
      "                      'conv_transpose_kernel_initializer_config': None,\n",
      "                      'custom_action_dist': None,\n",
      "                      'custom_model': None,\n",
      "                      'custom_model_config': {},\n",
      "                      'custom_preprocessor': None,\n",
      "                      'dim': 84,\n",
      "                      'encoder_latent_dim': None,\n",
      "                      'fcnet_activation': 'tanh',\n",
      "                      'fcnet_bias_initializer': None,\n",
      "                      'fcnet_bias_initializer_config': None,\n",
      "                      'fcnet_hiddens': [256, 256],\n",
      "                      'fcnet_weights_initializer': None,\n",
      "                      'fcnet_weights_initializer_config': None,\n",
      "                      'framestack': True,\n",
      "                      'free_log_std': False,\n",
      "                      'grayscale': False,\n",
      "                      'log_std_clip_param': 20.0,\n",
      "                      'lstm_bias_initializer': None,\n",
      "                      'lstm_bias_initializer_config': None,\n",
      "                      'lstm_cell_size': 256,\n",
      "                      'lstm_use_prev_action': False,\n",
      "                      'lstm_use_prev_action_reward': -1,\n",
      "                      'lstm_use_prev_reward': False,\n",
      "                      'lstm_weights_initializer': None,\n",
      "                      'lstm_weights_initializer_config': None,\n",
      "                      'max_seq_len': 20,\n",
      "                      'no_final_linear': False,\n",
      "                      'post_fcnet_activation': 'relu',\n",
      "                      'post_fcnet_bias_initializer': None,\n",
      "                      'post_fcnet_bias_initializer_config': None,\n",
      "                      'post_fcnet_hiddens': [],\n",
      "                      'post_fcnet_weights_initializer': None,\n",
      "                      'post_fcnet_weights_initializer_config': None,\n",
      "                      'use_attention': False,\n",
      "                      'use_lstm': False,\n",
      "                      'vf_share_layers': False,\n",
      "                      'zero_mean': True},\n",
      "            'normalize_actions': True,\n",
      "            'num_aggregator_actors_per_learner': 0,\n",
      "            'num_consecutive_env_runner_failures_tolerance': 100,\n",
      "            'num_cpus_for_main_process': 1,\n",
      "            'num_cpus_per_env_runner': 1,\n",
      "            'num_cpus_per_learner': 'auto',\n",
      "            'num_env_runners': 2,\n",
      "            'num_envs_per_env_runner': 1,\n",
      "            'num_epochs': 30,\n",
      "            'num_gpus': 0,\n",
      "            'num_gpus_per_env_runner': 0,\n",
      "            'num_gpus_per_learner': 0,\n",
      "            'num_learners': 0,\n",
      "            'observation_filter': 'NoFilter',\n",
      "            'observation_fn': None,\n",
      "            'observation_space': None,\n",
      "            'off_policy_estimation_methods': {},\n",
      "            'offline_data_class': None,\n",
      "            'offline_sampling': False,\n",
      "            'ope_split_batch_by_episode': True,\n",
      "            'optimizer': {},\n",
      "            'output': None,\n",
      "            'output_compress_columns': ['obs', 'new_obs'],\n",
      "            'output_config': {},\n",
      "            'output_filesystem': None,\n",
      "            'output_filesystem_kwargs': {},\n",
      "            'output_max_file_size': 67108864,\n",
      "            'output_max_rows_per_file': None,\n",
      "            'output_write_episodes': True,\n",
      "            'output_write_method': 'write_parquet',\n",
      "            'output_write_method_kwargs': {},\n",
      "            'output_write_remaining_data': False,\n",
      "            'placement_strategy': 'PACK',\n",
      "            'policies': {'default_policy': (None, None, None, None)},\n",
      "            'policies_to_train': None,\n",
      "            'policy_map_cache': -1,\n",
      "            'policy_map_capacity': 100,\n",
      "            'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x794a8db9aca0>,\n",
      "            'policy_states_are_swappable': False,\n",
      "            'postprocess_inputs': False,\n",
      "            'prelearner_buffer_class': None,\n",
      "            'prelearner_buffer_kwargs': {},\n",
      "            'prelearner_class': None,\n",
      "            'prelearner_module_synch_period': 10,\n",
      "            'preprocessor_pref': 'deepmind',\n",
      "            'remote_env_batch_wait_ms': 0,\n",
      "            'remote_worker_envs': False,\n",
      "            'render_env': False,\n",
      "            'replay_sequence_length': None,\n",
      "            'restart_failed_env_runners': True,\n",
      "            'restart_failed_sub_environments': False,\n",
      "            'rollout_fragment_length': 'auto',\n",
      "            'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
      "            'sample_timeout_s': 60.0,\n",
      "            'sampler_perf_stats_ema_coef': None,\n",
      "            'seed': None,\n",
      "            'sgd_minibatch_size': -1,\n",
      "            'shuffle_batch_per_epoch': True,\n",
      "            'shuffle_buffer_size': 0,\n",
      "            'simple_optimizer': False,\n",
      "            'sync_filters_on_rollout_workers_timeout_s': 10.0,\n",
      "            'synchronize_filters': -1,\n",
      "            'tf_session_args': {'allow_soft_placement': True,\n",
      "                                'device_count': {'CPU': 1},\n",
      "                                'gpu_options': {'allow_growth': True},\n",
      "                                'inter_op_parallelism_threads': 2,\n",
      "                                'intra_op_parallelism_threads': 2,\n",
      "                                'log_device_placement': False},\n",
      "            'torch_compile_learner': False,\n",
      "            'torch_compile_learner_dynamo_backend': 'inductor',\n",
      "            'torch_compile_learner_dynamo_mode': None,\n",
      "            'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
      "            'torch_compile_worker': False,\n",
      "            'torch_compile_worker_dynamo_backend': 'onnxrt',\n",
      "            'torch_compile_worker_dynamo_mode': None,\n",
      "            'torch_ddp_kwargs': {},\n",
      "            'torch_skip_nan_gradients': False,\n",
      "            'train_batch_size': 4000,\n",
      "            'update_worker_filter_stats': True,\n",
      "            'use_critic': True,\n",
      "            'use_gae': True,\n",
      "            'use_kl_loss': True,\n",
      "            'use_worker_filter_stats': True,\n",
      "            'validate_env_runners_after_construction': True,\n",
      "            'vf_clip_param': 10.0,\n",
      "            'vf_loss_coeff': 1.0,\n",
      "            'vf_share_layers': -1,\n",
      "            'worker_cls': -1},\n",
      " 'date': '2025-05-03_00-17-52',\n",
      " 'done': False,\n",
      " 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0},\n",
      " 'fault_tolerance': {'num_healthy_workers': 0, 'num_remote_worker_restarts': 0},\n",
      " 'hostname': 'jakub-Nitro-AN515-44',\n",
      " 'iterations_since_restore': 1,\n",
      " 'node_ip': '192.168.0.121',\n",
      " 'num_env_steps_sampled_lifetime': 0,\n",
      " 'num_training_step_calls_per_iteration': 1,\n",
      " 'perf': {'cpu_util_percent': np.float64(16.388764044943823),\n",
      "          'ram_util_percent': np.float64(47.469662921348316)},\n",
      " 'pid': 316406,\n",
      " 'time_since_restore': 61.77022194862366,\n",
      " 'time_this_iter_s': 61.77022194862366,\n",
      " 'time_total_s': 61.77022194862366,\n",
      " 'timers': {'env_runner_sampling_timer': 60.0094954160013,\n",
      "            'restore_env_runners': 2.723900252021849e-05,\n",
      "            'training_iteration': 60.00966674000665,\n",
      "            'training_step': 60.00951790600084},\n",
      " 'timestamp': 1746224272,\n",
      " 'training_iteration': 1,\n",
      " 'trial_id': 'default'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ray\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Configure PPO\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    # Use your registered environment\n",
    "    .environment(env=\"custom_env\", env_config={})\n",
    "    .framework(\"torch\")  # Use PyTorch or TensorFlow\n",
    "    .env_runners(num_cpus_per_env_runner=1)\n",
    "    .training(train_batch_size=4000, gamma=0.99)  # Training parameters\n",
    ")\n",
    "\n",
    "# Build the PPO algorithm\n",
    "algo = config.build()\n",
    "\n",
    "# Train the model\n",
    "for i in range(1):\n",
    "    result = algo.train()\n",
    "    pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the trained model\n",
    "# checkpoint = algo.save_to_path(\"\")\n",
    "# print(f\"Checkpoint saved at {checkpoint}\")\n",
    "\n",
    "# # Shutdown Ray\n",
    "# ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SingleAgentEnvRunner' object has no attribute 'get_policy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m total_reward = \u001b[32m0\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     action = \u001b[43malgo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_single_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     state, reward, done, _ = env.step(action)\n\u001b[32m     18\u001b[39m     total_reward += reward\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rllib/lib/python3.13/site-packages/ray/rllib/utils/deprecation.py:128\u001b[39m, in \u001b[36mDeprecated.<locals>._inner.<locals>._ctor\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m     deprecation_warning(\n\u001b[32m    122\u001b[39m         old=old \u001b[38;5;129;01mor\u001b[39;00m obj.\u001b[34m__name__\u001b[39m,\n\u001b[32m    123\u001b[39m         new=new,\n\u001b[32m    124\u001b[39m         help=help,\n\u001b[32m    125\u001b[39m         error=error,\n\u001b[32m    126\u001b[39m     )\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# Call the deprecated method/function.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rllib/lib/python3.13/site-packages/ray/rllib/algorithms/algorithm.py:3872\u001b[39m, in \u001b[36mAlgorithm.compute_single_action\u001b[39m\u001b[34m(self, observation, state, prev_action, prev_reward, info, input_dict, policy_id, full_fetch, explore, timestep, episode, unsquash_action, clip_action)\u001b[39m\n\u001b[32m   3869\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3870\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m observation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, err_msg\n\u001b[32m-> \u001b[39m\u001b[32m3872\u001b[39m policy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3873\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m policy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3874\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m   3875\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPolicyID \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpolicy_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not found in PolicyMap of the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3876\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAlgorithm\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms local worker!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3877\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rllib/lib/python3.13/site-packages/ray/rllib/algorithms/algorithm.py:2144\u001b[39m, in \u001b[36mAlgorithm.get_policy\u001b[39m\u001b[34m(self, policy_id)\u001b[39m\n\u001b[32m   2137\u001b[39m \u001b[38;5;129m@OldAPIStack\u001b[39m\n\u001b[32m   2138\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_policy\u001b[39m(\u001b[38;5;28mself\u001b[39m, policy_id: PolicyID = DEFAULT_POLICY_ID) -> Policy:\n\u001b[32m   2139\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return policy for the specified id, or None.\u001b[39;00m\n\u001b[32m   2140\u001b[39m \n\u001b[32m   2141\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   2142\u001b[39m \u001b[33;03m        policy_id: ID of the policy to return.\u001b[39;00m\n\u001b[32m   2143\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2144\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv_runner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_policy\u001b[49m(policy_id)\n",
      "\u001b[31mAttributeError\u001b[39m: 'SingleAgentEnvRunner' object has no attribute 'get_policy'"
     ]
    }
   ],
   "source": [
    "# Initialize Ray\n",
    "# ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# # Load the trained model\n",
    "# config = PPOConfig().environment(env=\"CartPole-v1\", env_config={}).framework(\"torch\")\n",
    "# algo = config.build_algo()\n",
    "# algo.restore()  # Path to the saved checkpoint\n",
    "\n",
    "# Evaluate the model\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = algo.compute_single_action(state)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "print(f\"Total reward during evaluation: {total_reward}\")\n",
    "\n",
    "# Shutdown Ray\n",
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
