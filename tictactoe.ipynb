{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeEnv(gym.Env):\n",
    "    def __init__(self, opponent: lambda obs: np.ndarray):\n",
    "        super(TicTacToeEnv, self).__init__()\n",
    "        self.action_space = gym.spaces.Discrete(9)\n",
    "        self.observation_space = gym.spaces.MultiDiscrete([3]*9)\n",
    "        self.state = np.zeros(9, dtype=int)\n",
    "        self.opponent = opponent\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.state = np.zeros(9, dtype=int)\n",
    "        return self.state, {}\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        # Model's move\n",
    "        #Punish model for invalid action\n",
    "        if self.state[action] != 0:\n",
    "            return self.state.copy(), -1, True, False, {}\n",
    "\n",
    "        # Make move and check for win\n",
    "        self.state[action] = 1\n",
    "        if self.checkState() is not None:\n",
    "            return self.state.copy(), self.checkState(), True, False, {}\n",
    "        \n",
    "        # Opponent's move\n",
    "        # Invert the state for opponent's perspective\n",
    "        state_for_opponent = self.getStateInversed()\n",
    "        opponent_action = self.opponent(state_for_opponent)\n",
    "        # If opponent's action is invalid, choose a random valid action\n",
    "        if self.state[opponent_action] != 0:\n",
    "            opponent_action = np.random.choice(np.where(self.state == 0)[0])\n",
    "        \n",
    "        self.state[opponent_action] = 2\n",
    "        if self.checkState() is not None:\n",
    "            return self.state.copy(), self.checkState(), True, False, {}\n",
    "\n",
    "        return self.state.copy(), 0, False, False, {}\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        if mode == 'human':\n",
    "            print(np.array(self.state).reshape(3, 3))\n",
    "\n",
    "\n",
    "    def checkState(self):\n",
    "        if np.any(np.all(self.state.reshape(3, 3) == 1, axis=0)) or \\\n",
    "           np.any(np.all(self.state.reshape(3, 3) == 1, axis=1)) or \\\n",
    "           np.all(np.diag(self.state.reshape(3, 3)) == 1) or \\\n",
    "           np.all(np.diag(np.fliplr(self.state.reshape(3, 3))) == 1):\n",
    "            return 1\n",
    "        if np.any(np.all(self.state.reshape(3, 3) == 2, axis=0)) or \\\n",
    "           np.any(np.all(self.state.reshape(3, 3) == 2, axis=1)) or \\\n",
    "           np.all(np.diag(self.state.reshape(3, 3)) == 2) or \\\n",
    "           np.all(np.diag(np.fliplr(self.state.reshape(3, 3))) == 2):\n",
    "            return -1\n",
    "        if not np.any(self.state == 0):\n",
    "            return 0\n",
    "        return None\n",
    "    \n",
    "\n",
    "    def getStateInversed(self):\n",
    "        state = np.zeros(9, dtype=int)\n",
    "        state[self.state == 1] = 2\n",
    "        state[self.state == 2] = 1\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "opponent = lambda obs: np.random.choice(np.where(obs == 0)[0])\n",
    "\n",
    "env = TicTacToeEnv(opponent)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 853  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 691         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012489583 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.19       |\n",
      "|    explained_variance   | -0.136      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0627      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    value_loss           | 0.325       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 665         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015353673 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.17       |\n",
      "|    explained_variance   | -0.0379     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.103       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    value_loss           | 0.399       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 651         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017099459 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.11       |\n",
      "|    explained_variance   | -0.00918    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.259       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0415     |\n",
      "|    value_loss           | 0.502       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 639         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020041682 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.03       |\n",
      "|    explained_variance   | 0.0133      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.274       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0452     |\n",
      "|    value_loss           | 0.568       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f89f42a7b10>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakub/miniconda3/envs/sb3/lib/python3.13/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83 +/- 0.5577633906953737\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=1000)\n",
    "print(f'{mean_reward} +/- {std_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As each step makes model's and opponent's move, on each render you see +2 moves. Not sure why last render returns zeros tho. Chat says that after returning done=True it is automatically reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2 0]\n",
      " [0 0 0]\n",
      " [1 0 0]]\n",
      "[[0 2 2]\n",
      " [0 1 0]\n",
      " [1 0 0]]\n",
      "[[1 2 2]\n",
      " [0 1 2]\n",
      " [1 0 0]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Ended with reward: [1.]\n",
      "[[0 0 0]\n",
      " [2 0 0]\n",
      " [1 0 0]]\n",
      "[[0 0 0]\n",
      " [2 0 2]\n",
      " [1 0 1]]\n",
      "[[2 0 1]\n",
      " [2 0 2]\n",
      " [1 0 1]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Ended with reward: [1.]\n",
      "[[0 2 0]\n",
      " [0 0 0]\n",
      " [1 0 0]]\n",
      "[[0 2 0]\n",
      " [0 1 0]\n",
      " [1 0 2]]\n",
      "[[2 2 0]\n",
      " [0 1 0]\n",
      " [1 1 2]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Ended with reward: [1.]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [1 2 0]]\n",
      "[[2 0 0]\n",
      " [0 1 0]\n",
      " [1 2 0]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Ended with reward: [1.]\n",
      "[[2 0 0]\n",
      " [0 0 0]\n",
      " [1 0 0]]\n",
      "[[2 2 1]\n",
      " [0 0 0]\n",
      " [1 0 0]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Ended with reward: [1.]\n",
      "[[0 2 0]\n",
      " [0 0 0]\n",
      " [1 0 0]]\n",
      "[[2 2 0]\n",
      " [0 1 0]\n",
      " [1 0 0]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Ended with reward: [1.]\n",
      "[[0 2 0]\n",
      " [0 0 0]\n",
      " [1 0 0]]\n",
      "[[0 2 0]\n",
      " [0 1 0]\n",
      " [1 2 0]]\n",
      "[[1 2 0]\n",
      " [0 1 2]\n",
      " [1 2 0]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Ended with reward: [1.]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [1 0 2]]\n",
      "[[2 0 1]\n",
      " [0 0 0]\n",
      " [1 0 2]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Ended with reward: [1.]\n",
      "[[0 0 0]\n",
      " [2 0 0]\n",
      " [1 0 0]]\n",
      "[[0 2 0]\n",
      " [2 0 0]\n",
      " [1 0 1]]\n",
      "[[0 2 2]\n",
      " [2 1 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Ended with reward: [1.]\n",
      "[[0 0 0]\n",
      " [0 0 2]\n",
      " [1 0 0]]\n",
      "[[0 0 0]\n",
      " [1 0 2]\n",
      " [1 0 2]]\n",
      "[[2 0 1]\n",
      " [1 0 2]\n",
      " [1 0 2]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Ended with reward: [1.]\n"
     ]
    }
   ],
   "source": [
    "vec_env = model.get_env()\n",
    "for i in range(10):\n",
    "    obs = vec_env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = vec_env.step(action)\n",
    "        vec_env.envs[0].render('human')\n",
    "    print(f\"Ended with reward: {reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model on previous models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make model better lets train it against previous model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "def opponent(obs): return np.random.choice(np.where(obs == 0)[0])\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with switching models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 939  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 757         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009481499 |\n",
      "|    clip_fraction        | 0.0609      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.19       |\n",
      "|    explained_variance   | -0.286      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0431      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    value_loss           | 0.304       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 708         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013655643 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.17       |\n",
      "|    explained_variance   | -0.0365     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0814      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    value_loss           | 0.337       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 679         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017463438 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.13       |\n",
      "|    explained_variance   | -0.00472    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.246       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0387     |\n",
      "|    value_loss           | 0.461       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 656         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016800385 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.06       |\n",
      "|    explained_variance   | -0.00791    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.231       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0417     |\n",
      "|    value_loss           | 0.622       |\n",
      "-----------------------------------------\n",
      "Using cpu device\n",
      "0.6 +/- 0.8\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 854  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 679         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018970843 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.88       |\n",
      "|    explained_variance   | 0.0203      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.334       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0472     |\n",
      "|    value_loss           | 0.792       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 637         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016620118 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.79       |\n",
      "|    explained_variance   | 0.00321     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.339       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0383     |\n",
      "|    value_loss           | 0.805       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 636         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015993617 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.68       |\n",
      "|    explained_variance   | -0.00699    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.364       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0369     |\n",
      "|    value_loss           | 0.811       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 631         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015358727 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.0349      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.315       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0378     |\n",
      "|    value_loss           | 0.737       |\n",
      "-----------------------------------------\n",
      "Using cpu device\n",
      "1.0 +/- 0.0\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 905  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 719         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012649987 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.0452      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.287       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    value_loss           | 0.626       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 666         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015163884 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.0838      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.208       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0328     |\n",
      "|    value_loss           | 0.586       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 655         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016063478 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.0795      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.197       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    value_loss           | 0.548       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 652         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013700477 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.175       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    value_loss           | 0.495       |\n",
      "-----------------------------------------\n",
      "Using cpu device\n",
      "1.0 +/- 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    env = TicTacToeEnv(opponent)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "\n",
    "    model.learn(total_timesteps=10_000)\n",
    "\n",
    "    prev_model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "    prev_model.set_parameters(model.get_parameters())\n",
    "    def opponent(obs): return prev_model.predict(obs, deterministic=True)[0]\n",
    "\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "    print(f'{mean_reward} +/- {std_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see model found optimal win strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakub/miniconda3/envs/sb3/lib/python3.13/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 +/- 0.0\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=1000)\n",
    "print(f'{mean_reward} +/- {std_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2 0]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n",
      "[[0 2 0]\n",
      " [0 1 0]\n",
      " [0 2 1]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Ended with reward: [1.]\n",
      "[[0 0 0]\n",
      " [0 1 0]\n",
      " [0 0 2]]\n",
      "[[0 0 1]\n",
      " [0 1 0]\n",
      " [0 2 2]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Ended with reward: [1.]\n",
      "[[0 0 0]\n",
      " [0 1 0]\n",
      " [0 2 0]]\n",
      "[[2 0 0]\n",
      " [0 1 0]\n",
      " [0 2 1]]\n",
      "[[2 0 1]\n",
      " [0 1 0]\n",
      " [2 2 1]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Ended with reward: [1.]\n",
      "[[0 0 0]\n",
      " [0 1 0]\n",
      " [0 2 0]]\n",
      "[[2 0 0]\n",
      " [0 1 0]\n",
      " [0 2 1]]\n",
      "[[2 0 1]\n",
      " [2 1 0]\n",
      " [0 2 1]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Ended with reward: [1.]\n",
      "[[0 0 0]\n",
      " [0 1 0]\n",
      " [2 0 0]]\n",
      "[[0 0 0]\n",
      " [0 1 0]\n",
      " [2 2 1]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Ended with reward: [1.]\n",
      "[[0 0 0]\n",
      " [0 1 2]\n",
      " [0 0 0]]\n",
      "[[0 0 0]\n",
      " [0 1 2]\n",
      " [0 2 1]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Ended with reward: [1.]\n",
      "[[0 0 0]\n",
      " [0 1 0]\n",
      " [0 0 2]]\n",
      "[[2 0 1]\n",
      " [0 1 0]\n",
      " [0 0 2]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Ended with reward: [1.]\n",
      "[[0 0 0]\n",
      " [0 1 0]\n",
      " [2 0 0]]\n",
      "[[0 0 0]\n",
      " [2 1 0]\n",
      " [2 0 1]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Ended with reward: [1.]\n",
      "[[0 0 0]\n",
      " [0 1 0]\n",
      " [0 2 0]]\n",
      "[[0 2 0]\n",
      " [0 1 0]\n",
      " [0 2 1]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Ended with reward: [1.]\n",
      "[[0 0 2]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n",
      "[[1 0 2]\n",
      " [0 1 0]\n",
      " [0 0 2]]\n",
      "[[1 2 2]\n",
      " [1 1 0]\n",
      " [0 0 2]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Ended with reward: [1.]\n"
     ]
    }
   ],
   "source": [
    "vec_env = model.get_env()\n",
    "for i in range(10):\n",
    "    obs = vec_env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = vec_env.step(action)\n",
    "        vec_env.envs[0].render('human')\n",
    "    print(f\"Ended with reward: {reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
