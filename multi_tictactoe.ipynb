{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.13.2)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import pygame\n",
    "\n",
    "import numpy as np\n",
    "from numpy import copy\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import EzPickle\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import CnnPolicy, MlpPolicy\n",
    "\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "\n",
    "import pettingzoo\n",
    "from pettingzoo import ParallelEnv, AECEnv\n",
    "from pettingzoo.utils import wrappers\n",
    "from pettingzoo.utils.agent_selector import agent_selector\n",
    "from pettingzoo.classic import connect_four_v3\n",
    "from pettingzoo.butterfly import knights_archers_zombies_v10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SB3ActionMaskWrapper(pettingzoo.utils.BaseWrapper, gym.Env):\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed, options)\n",
    "\n",
    "        self.observation_space = super().observation_space(self.possible_agents[0])[\"observation\"]\n",
    "        self.action_space = super().action_space(self.possible_agents[0])\n",
    "\n",
    "        return self.observe(self.agent_selection), {}\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        current_agent = self.agent_selection\n",
    "\n",
    "        super().step(action)\n",
    "\n",
    "        next_agent = self.agent_selection\n",
    "        return (\n",
    "            self.observe(next_agent),\n",
    "            self._cumulative_rewards[current_agent],\n",
    "            self.terminations[current_agent],\n",
    "            self.truncations[current_agent],\n",
    "            self.infos[current_agent],\n",
    "        )\n",
    "\n",
    "\n",
    "    def observe(self, agent):\n",
    "        return super().observe(agent)[\"observation\"]\n",
    "\n",
    "\n",
    "    def action_mask(self):\n",
    "        return super().observe(self.agent_selection)[\"action_mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeAecEnv(AECEnv, EzPickle):\n",
    "    metadata = {\n",
    "        \"name\": \"tttAec-v0\",\n",
    "        \"is_parallelizable\": False,\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.board = np.zeros(9, dtype=np.int8)\n",
    "\n",
    "        self.agents = [\"p0\", \"p1\"]\n",
    "        self.possible_agents = self.agents[:]\n",
    "\n",
    "        self.action_spaces = {i: spaces.Discrete(9) for i in self.agents}\n",
    "        self.observation_spaces = {\n",
    "            i: spaces.Dict({\n",
    "                \"observation\": spaces.Box(low=0, high=1, shape=(3, 3, 2), dtype=np.int8),\n",
    "                \"action_mask\": spaces.Box(low=0, high=1, shape=(3, 3), dtype=np.int8),\n",
    "            })\n",
    "            for i in self.agents\n",
    "        }\n",
    "\n",
    "\n",
    "    def observe(self, agent):\n",
    "        board_vals = np.array(self.board).reshape(3, 3)\n",
    "        cur_player = self.possible_agents.index(agent)\n",
    "        opp_player = (cur_player + 1) % 2\n",
    "\n",
    "        cur_p_board = np.equal(board_vals, cur_player + 1)\n",
    "        opp_p_board = np.equal(board_vals, opp_player + 1)\n",
    "\n",
    "        observation = np.stack([cur_p_board, opp_p_board], axis=2).astype(np.int8)\n",
    "        legal_moves = self._legal_moves() if agent == self.agent_selection else []\n",
    "\n",
    "        action_mask = np.zeros(9, \"int8\")\n",
    "        for i in legal_moves:\n",
    "            action_mask[i] = 1\n",
    "\n",
    "        return {\"observation\": observation, \"action_mask\": action_mask}\n",
    "\n",
    "\n",
    "    def _legal_moves(self):\n",
    "        return [i for i in range(9) if self.board[i] == 0]\n",
    "\n",
    "\n",
    "    def observation_space(self, agent):\n",
    "        return self.observation_spaces[agent]\n",
    "\n",
    "\n",
    "    def action_space(self, agent):\n",
    "        return self.action_spaces[agent]\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        if (\n",
    "            self.truncations[self.agent_selection]\n",
    "            or self.terminations[self.agent_selection]\n",
    "        ):\n",
    "            print(f\"Agent {self.agent_selection} tried to step in a terminated or truncated state.\")\n",
    "            return self._was_dead_step(action)\n",
    "\n",
    "        assert self.board[0:9][action] == 0, \"played illegal move.\"\n",
    "\n",
    "        piece = self.agents.index(self.agent_selection) + 1\n",
    "        self.board[action] = piece\n",
    "\n",
    "        next_agent = self._agent_selector.next()\n",
    "\n",
    "        winner = self.check_for_winner()\n",
    "\n",
    "        if winner:\n",
    "            self.rewards[self.agent_selection] += 1\n",
    "            self.rewards[next_agent] -= 1\n",
    "            self.terminations = {i: True for i in self.agents}\n",
    "        elif not any(x == 0 for x in self.board):\n",
    "            self.terminations = {i: True for i in self.agents}\n",
    "\n",
    "        self.agent_selection = next_agent\n",
    "        self._accumulate_rewards()\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.board = np.zeros(9, dtype=np.int8)\n",
    "\n",
    "        self.agents = self.possible_agents[:]\n",
    "        self.rewards = {i: 0 for i in self.agents}\n",
    "        self._cumulative_rewards = {name: 0 for name in self.agents}\n",
    "        self.terminations = {i: False for i in self.agents}\n",
    "        self.truncations = {i: False for i in self.agents}\n",
    "        self.infos = {i: {} for i in self.agents}\n",
    "\n",
    "        self._agent_selector = agent_selector(self.agents)\n",
    "\n",
    "        self.agent_selection = self._agent_selector.reset()\n",
    "\n",
    "\n",
    "    def check_for_winner(self):\n",
    "        board = np.reshape(self.board, (3, 3))\n",
    "        for i in range(3):\n",
    "            if np.all(board[i, :] == 1) or np.all(board[:, i] == 1):\n",
    "                return True\n",
    "            if np.all(board[i, :] == 2) or np.all(board[:, i] == 2):\n",
    "                return True\n",
    "        if np.all(np.diag(board) == 1) or np.all(np.diag(np.fliplr(board)) == 1):\n",
    "            return True\n",
    "        if np.all(np.diag(board) == 2) or np.all(np.diag(np.fliplr(board)) == 2):\n",
    "            return True\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_fn(env: TicTacToeAecEnv):\n",
    "    return env.action_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TicTacToeAecEnv()\n",
    "env = wrappers.OrderEnforcingWrapper(env)\n",
    "env = SB3ActionMaskWrapper(env)\n",
    "env.reset(seed=np.random.randint(0, 1000))\n",
    "env = ActionMasker(env, mask_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = MaskablePPO(\n",
    "    MaskableActorCriticPolicy, \n",
    "    env, \n",
    "    verbose=1,\n",
    "    ent_coef=0.01,\n",
    ")\n",
    "model.set_random_seed(seed=np.random.randint(0, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.78     |\n",
      "|    ep_rew_mean     | 0.88     |\n",
      "| time/              |          |\n",
      "|    fps             | 416      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.44        |\n",
      "|    ep_rew_mean          | 0.93        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 347         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011186462 |\n",
      "|    clip_fraction        | 0.0803      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | -0.532      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00102    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    value_loss           | 0.118       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.34        |\n",
      "|    ep_rew_mean          | 0.94        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 329         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011282951 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | -0.0734     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.026      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    value_loss           | 0.0537      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.15        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 325         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011755538 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00078    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    value_loss           | 0.0249      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.07        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 319         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011212724 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | -4.54       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0186     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    value_loss           | 0.00262     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sb3_contrib.ppo_mask.ppo_mask.MaskablePPO at 0x77b84b37af90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 1]]\n",
      "\n",
      "[[0 0 0]\n",
      " [2 0 0]\n",
      " [0 0 1]]\n",
      "\n",
      "[[0 0 0]\n",
      " [2 0 0]\n",
      " [1 0 1]]\n",
      "\n",
      "[[0 0 0]\n",
      " [2 0 0]\n",
      " [1 2 1]]\n",
      "\n",
      "[[1 0 0]\n",
      " [2 0 0]\n",
      " [1 2 1]]\n",
      "\n",
      "[[1 0 0]\n",
      " [2 0 2]\n",
      " [1 2 1]]\n",
      "\n",
      "[[1 0 1]\n",
      " [2 0 2]\n",
      " [1 2 1]]\n",
      "\n",
      "[[1 2 1]\n",
      " [2 0 2]\n",
      " [1 2 1]]\n",
      "\n",
      "[[1 2 1]\n",
      " [2 1 2]\n",
      " [1 2 1]]\n",
      "Game ended. Reward: p1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "obs, info = env.reset(seed=np.random.randint(0, 1000))\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    # If obs is not a dictionary, assume it directly contains the observation\n",
    "    if isinstance(obs, dict):\n",
    "        observation, action_mask = obs[\"observation\"], obs[\"action_mask\"]\n",
    "    else:\n",
    "        observation = obs\n",
    "        action_mask = env.env.action_mask()  # Retrieve the action mask separately\n",
    "\n",
    "    action, _states = model.predict(observation, action_masks=action_mask)\n",
    "\n",
    "    obs, reward, termination, truncation, info = env.step(action)\n",
    "\n",
    "    print(env.env.board.reshape(3, 3))\n",
    "\n",
    "    done = termination or truncation\n",
    "    if done:\n",
    "        print(f\"Game ended. Reward: {env.env.agent_selection}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are playing as Player 0 (p0). The model is Player 1 (p1).\n",
      "Board positions are numbered as follows:\n",
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n",
      "\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Your turn!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n",
      "Model's turn...\n",
      "[[0 0 0]\n",
      " [0 1 0]\n",
      " [2 0 0]]\n",
      "Your turn!\n",
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [2 0 0]]\n",
      "Model's turn...\n",
      "[[1 0 2]\n",
      " [0 1 0]\n",
      " [2 0 0]]\n",
      "Your turn!\n",
      "[[1 0 2]\n",
      " [0 1 0]\n",
      " [2 0 1]]\n",
      "Game over!\n",
      "You win!\n"
     ]
    }
   ],
   "source": [
    "obs, info = env.reset(seed=np.random.randint(0, 1000))\n",
    "done = False\n",
    "\n",
    "print(\"You are playing as Player 0 (p0). The model is Player 1 (p1).\")\n",
    "print(\"Board positions are numbered as follows:\")\n",
    "print(np.arange(9).reshape(3, 3))  # Display board positions for reference\n",
    "print()\n",
    "print(env.env.board.reshape(3, 3), flush=True)  # Display initial board state\n",
    "\n",
    "while not done:\n",
    "    # If obs is not a dictionary, assume it directly contains the observation\n",
    "    if isinstance(obs, dict):\n",
    "        observation, action_mask = obs[\"observation\"], obs[\"action_mask\"]\n",
    "    else:\n",
    "        observation = obs\n",
    "        action_mask = env.env.action_mask()  # Retrieve the action mask separately\n",
    "\n",
    "    # Determine the current agent\n",
    "    current_agent = env.env.agent_selection\n",
    "\n",
    "    if current_agent == \"p0\":  # Human player's turn\n",
    "        print(\"Your turn!\", flush=True)\n",
    "\n",
    "        inp = input(\"Enter your move (0-8): \")\n",
    "        if inp.isdigit() and int(inp) in range(9) and action_mask[int(inp)] == 1:\n",
    "            action = int(inp)\n",
    "        else:\n",
    "            print(\"Invalid move\")\n",
    "            break\n",
    "\n",
    "    else:  # Model's turn\n",
    "        print(\"Model's turn...\", flush=True)\n",
    "        action, _states = model.predict(\n",
    "            observation, action_masks=action_mask, deterministic=True\n",
    "        )\n",
    "\n",
    "    # Take the chosen action\n",
    "    obs, reward, termination, truncation, info = env.step(action)\n",
    "\n",
    "    # Print the updated board state\n",
    "    print(env.env.board.reshape(3, 3), flush=True)  # Display the board state after the move\n",
    "\n",
    "    # Check if the game has ended\n",
    "    done = termination or truncation\n",
    "    if done:\n",
    "        print(\"Game over!\")\n",
    "        if reward > 0:\n",
    "            if current_agent == \"p0\":\n",
    "                print(\"You win!\")\n",
    "            else:\n",
    "                print(\"The model wins!\")\n",
    "        elif reward < 0:\n",
    "            if current_agent == \"p0\":\n",
    "                print(\"The model wins!\")\n",
    "            else:\n",
    "                print(\"You win!\")\n",
    "        else:\n",
    "            print(\"It's a draw!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb3multi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
